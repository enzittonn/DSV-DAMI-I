{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello! This lab has an objective to teach various ways of evaluating a model we already learned how to build in Lab 3. In the last lab, we did not perform any automatic process to determine the optimal parameter, but just put any value and saw whether the model showed a better perfor|mance. In this lab, you will learn how to perform a grid search for parameter settings. After that, in the following assignment, you will try various ways of evaluating a model. This lab contains the extra tasks to practice **after** completing the labs with videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4-3. Run more algorithms using scikit-learn\n",
    "  - Hold-out\n",
    "  - Repeated hold-out\n",
    "\n",
    "- 4-4. Implement manually\n",
    "  - Grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-3.  Run more algorithms using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn. You will try two more methods related to splitting the datasets. After splitting, you will try to run a simple SVM method using the split datasets. Throughout this section, you will use the same training and test dataset we prepared above in the lab session! If you want to create it again, you can run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "RANDOM_SEED = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sonar.all-data\", header=None)\n",
    "X = data.drop(60, axis=1)\n",
    "y = data[60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normal holdout means splitting the dataset into two parts: A training set and a test set. Therefore, this method cannot be used when the task involves parameter tuning. In this case, we should use pre-defined parameters not based on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already tried it once in the previous lab. Scikit-learn supports holdout method by **train_test_split** method in **model_selection** package. Let's load it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly run this method using our dataset (`X, y`). Let's set the test size to 0.2 (test_size), and apply stratifying. Do not forget to set *random_state* to our `RANDOM_SEED` defined above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an SVC instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the model with a *training* dataset and its label!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return the score with the fitted model and the *test* dataset and its label! Let's assign the value into the value called `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeated holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A repeated holdout is somewhat similar to k-fold cross-validation. You can even say that it is a more generalized version of k-fold. It is running the normal holdout test multiple times by shuffling the dataset for each trial. In this way, you can come up with a more reasonable performance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The repeated holdout is supported by the class called **ShuffleSplit** located in the **model_selection** package. Let's load it first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShuffleSplit method receives the following parameters: number of splits (`n_splits`), the proportion of a test set (`test_size`), and random state (`random_state`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = ShuffleSplit(n_splits=5, test_size=.20, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this round, let's run the **cross_val_score** method we learned above. You can use this method with ShuffleSplit instance together. You may find the information somewhere in this lab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76190476, 0.85714286, 0.73809524, 0.78571429, 0.83333333])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(svc, X, y, cv=rs)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a maximum score and mean cross-validation score and save them into corresponding variables here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = np.max(scores)\n",
    "mean_score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-4. Implement manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your last extra work in this lab is to try implementing grid search. For simplicity, we will not require you to develop cross-validation inside, since the core function of grid-search is to find out optimal parameter given the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pre-made the skeleton structure for you. You should develop *fit* and two member variables (*best_estimator_, best_score_*). You may want to use Python's `itertools.product` if you need to make your work easier. To evaluate each parameter, you should use **StratifiedKFold** inside to get the same result with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we give you a parameter grid variable that was used above in our lab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will again stick to a simple SVC classifier. But we do not need to initialize it now since we may need different instances for each iteration of parameter combinations. We used **class** notation and use **self** variable inside. This structure is made to give you the same experience with scikit-learn when testing. You only use **self** here when you want to call the methods defined in the class structure or want to access the class variable defined by self first. The class-based structure will not appear in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearch_Manual():\n",
    "    def __init__(self, estimator, param_grid, cv):\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.best_estimator_ = None\n",
    "        self.best_score_ = 0\n",
    "        self.cv = cv\n",
    "        self.skf = StratifiedKFold(n_splits=cv)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        for param_set in self.param_grid:\n",
    "            keys, values = zip(*param_set.items())\n",
    "            permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "            \n",
    "            for param in permutations_dicts:\n",
    "                classifier = self.estimator(**param)\n",
    "                scores = []\n",
    "                for train_index, test_index in skf.split(X, y):\n",
    "                    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                    \n",
    "                    classifier.fit(X_train, y_train)\n",
    "                    scores.append(classifier.score(X_test, y_test))\n",
    "                \n",
    "                mean_score = np.mean(scores)\n",
    "                if mean_score > self.best_score_:\n",
    "                    self.best_estimator_ = classifier\n",
    "                    self.best_score_ = mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we made some callers for you. You should be able to initialize your instance in the same way as scikit-learn's **GridSearchCV** works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearch_Manual(estimator=SVC, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to fit and return the scores in the same way too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'skf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-b1092c9d9d76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-259c01175afc>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'skf' is not defined"
     ]
    }
   ],
   "source": [
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extra task is not graded, but let's check that your result is the same with the one from scikit_learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5)\n",
    "gs.fit(X, y)\n",
    "print(grid.best_score_, grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of practice assignment 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
